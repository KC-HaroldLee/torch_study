{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import\n",
    "import os\n",
    "import shutil\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "datasets_dir = './datasets/plant/'\n",
    "classes_list = os.listdir(datasets_dir)\n",
    "\n",
    "base_dir = './datasets/plant_splitted/'\n",
    "\n",
    "train_dir = base_dir + 'train'\n",
    "val_dir = base_dir + 'val'\n",
    "test_dir = base_dir + 'test'\n",
    "print(os.path.isdir(train_dir))\n",
    "print(os.path.isdir(val_dir))\n",
    "print(os.path.isdir(test_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for class_name in classes_list :\n",
    "#     os.mkdir(os.path.join(train_dir, class_name))\n",
    "#     os.mkdir(os.path.join(val_dir, class_name))\n",
    "#     os.mkdir(os.path.join(test_dir, class_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for class_name in classes_list :\n",
    "#     path = os.path.join(datasets_dir, class_name)\n",
    "#     file_name_list = os.listdir(path)\n",
    "\n",
    "#     train_size = math.floor(len(file_name_list) * 0.6)\n",
    "#     val_size = math.floor(len(file_name_list) * 0.2)\n",
    "#     test_size = math.floor(len(file_name_list) * 0.2)\n",
    "\n",
    "#     # print(train_size, val_size, test_size)\n",
    "\n",
    "#     # 이렇게 구분하는 방법도 있군\n",
    "#     train_file_name_list = file_name_list[:train_size]\n",
    "#     train_class_folder_path = os.path.join(train_dir, class_name)\n",
    "#     # print('train size-[{}]:{}'.format(class_name, len(train_file_name_list)))\n",
    "#     # print(len(train_file_name_list))\n",
    "#     for train_file_name in train_file_name_list :\n",
    "#         src_file = os.path.join(path, train_file_name)\n",
    "#         dst_file = os.path.join(train_class_folder_path, train_file_name)\n",
    "#         shutil.copyfile(src_file, dst_file)\n",
    "\n",
    "\n",
    "#     val_file_name_list = file_name_list[train_size:train_size+val_size] # 신박하네 ㅋㅋ\n",
    "#     val_class_folder_path = os.path.join(val_dir, class_name)\n",
    "#     # print('val size-[{}]:{}'.format(class_name, len(val_file_name_list)))\n",
    "#     # print(len(val_file_name_list))\n",
    "#     for val_file_name in val_file_name_list :\n",
    "#         src_file = os.path.join(path, val_file_name)\n",
    "#         dst_file = os.path.join(val_class_folder_path, val_file_name)\n",
    "#         shutil.copyfile(src_file, dst_file)\n",
    "\n",
    "\n",
    "#     test_file_name_list = file_name_list[train_size+val_size:train_size+val_size+test_size] # 파일 개수가 많지 않겠군\n",
    "#     test_class_folder_path = os.path.join(test_dir, class_name)\n",
    "#     # print('test size-[{}]:{}'.format(class_name, len(test_file_name_list)))\n",
    "#     # print(len(test_file_name_list))\n",
    "#     for test_file_name in test_file_name_list :\n",
    "#         src_file = os.path.join(path, test_file_name)\n",
    "#         dst_file = os.path.join(test_class_folder_path, test_file_name)\n",
    "#         shutil.copyfile(src_file, dst_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.12.0.dev20220505+cu116'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "USE_CUDA = torch.cuda.is_available()\n",
    "DEVICE = torch.device('cuda' if USE_CUDA else 'cpu')\n",
    "\n",
    "BATCH_SIZE = 1024\n",
    "EPOCH = 30\n",
    "\n",
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DEVICE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 베이스라인 모델 학습을 위한 준비\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.datasets import ImageFolder # 무슨 모듈?\n",
    "\n",
    "# compose는 이미지 전처리, 증강등에 이용되는 메소드이다. \n",
    "# 이미지 크기, 그리고 데이터형식(Tensor형태)로 변환해 준다.\n",
    "transform_base = transforms.Compose([transforms.Resize((64,64)), transforms.ToTensor()]) # 형식을 지정하는 듯 하다\n",
    "\n",
    "# 폴더 이름을 클래스이름으로 사용할 때 ImageFolder를 사용한다. transform은 데이터를 불러온 후 전처리, 증강을 위한 방법을 지정한다. 앞에서 정의한 것을 지정한다\n",
    "train_ds = ImageFolder(root='datasets/plant_splitted/train/', transform=transform_base)\n",
    "val_ds = ImageFolder(root='datasets/plant_splitted/val/', transform=transform_base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# train_loader = torch.utils.data.DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, num_workers=4) # 오 워커까지\n",
    "\n",
    "# val_loader = torch.utils.data.DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=True, num_workers=4) # 오 워커까지\n",
    "train_loader = torch.utils.data.DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, num_workers=4)\n",
    "val_loader = torch.utils.data.DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=True, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 설계\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(3, 32, 3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, padding=1)\n",
    "        self.conv3 = nn.Conv2d(64, 64, 3, padding=1)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "\n",
    "        self.fc1 = nn.Linear(4096, 512) #Dense!?\n",
    "        self.fc2 = nn.Linear(512, len(classes_list)) #ㅋㅋㅋㅋㅋㅋ\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x) # 액티베이션 레이어구먼 스위시가 없네\n",
    "        x = self.pool(x)\n",
    "        x = F.dropout(x, p=0.25, training=self.training) # 여기서의 self.training은 상속 받았음(nn.Module)\n",
    "\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.pool(x)\n",
    "        x = F.dropout(x, p=0.25, training=self.training)\n",
    "    \n",
    "        x = self.conv3(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.pool(x)\n",
    "        x = F.dropout(x, p=0.25, training=self.training)\n",
    "\n",
    "        x = x.view(-1, 4096) # 플랫튼 같은데...\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        return F.log_softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = Net().to(DEVICE)\n",
    "optimizer = optim.Adam(base_model.parameters(), lr=0.001) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 트레이닝... 근데 val이없음\n",
    "def train(model, train_loader, optimizer):\n",
    "    model.train() # nn.Module 에서 상속 받음\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(DEVICE), target.to(DEVICE)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = F.cross_entropy(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step() # 파라미터에 할당된 Gradient값을 이용해 모델의 파라미터를 업데이트한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, test_loader):\n",
    "    model.eval()  \n",
    "    test_loss = 0 \n",
    "    correct = 0   \n",
    "    \n",
    "    with torch.no_grad(): \n",
    "        for data, target in test_loader:  \n",
    "            data, target = data.to(DEVICE), target.to(DEVICE)  \n",
    "            output = model(data) \n",
    "            \n",
    "            test_loss += F.cross_entropy(output,target, reduction='sum').item() \n",
    " \n",
    "            \n",
    "            pred = output.max(1, keepdim=True)[1]\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item() \n",
    "   \n",
    "    test_loss /= len(test_loader.dataset) \n",
    "    test_accuracy = 100. * correct / len(test_loader.dataset) \n",
    "    return test_loss, test_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델학습 실행하기\n",
    "def train_baseline(model ,train_loader, val_loader, optimizer, num_epochs = 30): # 여기 있군 로더\n",
    "    best_acc = 0.0  \n",
    "    best_model_wts = copy.deepcopy(model.state_dict()) # 가중치는 여기잇다.\n",
    "  \n",
    "    for epoch in range(1, num_epochs + 1): #프린트를 위한 시프트\n",
    "        since = time.time()  \n",
    "        train(model, train_loader, optimizer)\n",
    "        train_loss, train_acc = evaluate(model, train_loader) \n",
    "        val_loss, val_acc = evaluate(model, val_loader)\n",
    "        \n",
    "        if val_acc > best_acc: \n",
    "            best_acc = val_acc \n",
    "            best_model_wts = copy.deepcopy(model.state_dict()) \n",
    "        \n",
    "        time_elapsed = time.time() - since \n",
    "        print('-------------- epoch {} ----------------'.format(epoch))\n",
    "        print('train Loss: {:.4f}, Accuracy: {:.2f}%'.format(train_loss, train_acc))   \n",
    "        print('val Loss: {:.4f}, Accuracy: {:.2f}%'.format(val_loss, val_acc))\n",
    "        print('Completed in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60)) \n",
    "    model.load_state_dict(best_model_wts)  # 가중치 가져오기\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------- epoch 1 ----------------\n",
      "train Loss: 2.6299, Accuracy: 28.44%\n",
      "val Loss: 2.6266, Accuracy: 29.00%\n",
      "Completed in 0m 46s\n",
      "-------------- epoch 2 ----------------\n",
      "train Loss: 1.5689, Accuracy: 56.46%\n",
      "val Loss: 1.5843, Accuracy: 55.93%\n",
      "Completed in 0m 32s\n",
      "-------------- epoch 3 ----------------\n",
      "train Loss: 1.2000, Accuracy: 65.53%\n",
      "val Loss: 1.2271, Accuracy: 63.98%\n",
      "Completed in 0m 31s\n",
      "-------------- epoch 4 ----------------\n",
      "train Loss: 0.8690, Accuracy: 74.39%\n",
      "val Loss: 0.9006, Accuracy: 73.08%\n",
      "Completed in 0m 31s\n",
      "-------------- epoch 5 ----------------\n",
      "train Loss: 0.7314, Accuracy: 77.82%\n",
      "val Loss: 0.7715, Accuracy: 76.25%\n",
      "Completed in 0m 31s\n",
      "-------------- epoch 6 ----------------\n",
      "train Loss: 0.6040, Accuracy: 82.27%\n",
      "val Loss: 0.6483, Accuracy: 80.50%\n",
      "Completed in 0m 36s\n",
      "-------------- epoch 7 ----------------\n",
      "train Loss: 0.5830, Accuracy: 82.16%\n",
      "val Loss: 0.6360, Accuracy: 80.29%\n",
      "Completed in 0m 35s\n",
      "-------------- epoch 8 ----------------\n",
      "train Loss: 0.4798, Accuracy: 85.50%\n",
      "val Loss: 0.5393, Accuracy: 83.22%\n",
      "Completed in 0m 36s\n",
      "-------------- epoch 9 ----------------\n",
      "train Loss: 0.4400, Accuracy: 86.97%\n",
      "val Loss: 0.4965, Accuracy: 84.43%\n",
      "Completed in 0m 36s\n",
      "-------------- epoch 10 ----------------\n",
      "train Loss: 0.4112, Accuracy: 87.48%\n",
      "val Loss: 0.4722, Accuracy: 85.10%\n",
      "Completed in 0m 36s\n",
      "-------------- epoch 11 ----------------\n",
      "train Loss: 0.4207, Accuracy: 86.85%\n",
      "val Loss: 0.4887, Accuracy: 84.45%\n",
      "Completed in 0m 34s\n",
      "-------------- epoch 12 ----------------\n",
      "train Loss: 0.3800, Accuracy: 88.30%\n",
      "val Loss: 0.4540, Accuracy: 85.47%\n",
      "Completed in 0m 32s\n",
      "-------------- epoch 13 ----------------\n",
      "train Loss: 0.3179, Accuracy: 90.26%\n",
      "val Loss: 0.3919, Accuracy: 87.44%\n",
      "Completed in 0m 31s\n",
      "-------------- epoch 14 ----------------\n",
      "train Loss: 0.3136, Accuracy: 90.46%\n",
      "val Loss: 0.3901, Accuracy: 87.23%\n",
      "Completed in 0m 31s\n",
      "-------------- epoch 15 ----------------\n",
      "train Loss: 0.2814, Accuracy: 91.32%\n",
      "val Loss: 0.3626, Accuracy: 87.92%\n",
      "Completed in 0m 31s\n",
      "-------------- epoch 16 ----------------\n",
      "train Loss: 0.2688, Accuracy: 91.91%\n",
      "val Loss: 0.3501, Accuracy: 88.68%\n",
      "Completed in 0m 31s\n",
      "-------------- epoch 17 ----------------\n",
      "train Loss: 0.2360, Accuracy: 93.24%\n",
      "val Loss: 0.3205, Accuracy: 89.74%\n",
      "Completed in 0m 30s\n",
      "-------------- epoch 18 ----------------\n",
      "train Loss: 0.2166, Accuracy: 93.61%\n",
      "val Loss: 0.3071, Accuracy: 90.25%\n",
      "Completed in 0m 31s\n",
      "-------------- epoch 19 ----------------\n",
      "train Loss: 0.2280, Accuracy: 93.23%\n",
      "val Loss: 0.3211, Accuracy: 89.84%\n",
      "Completed in 0m 31s\n",
      "-------------- epoch 20 ----------------\n",
      "train Loss: 0.2043, Accuracy: 93.95%\n",
      "val Loss: 0.3037, Accuracy: 90.35%\n",
      "Completed in 0m 31s\n",
      "-------------- epoch 21 ----------------\n",
      "train Loss: 0.1726, Accuracy: 95.03%\n",
      "val Loss: 0.2711, Accuracy: 91.50%\n",
      "Completed in 0m 30s\n",
      "-------------- epoch 22 ----------------\n",
      "train Loss: 0.1713, Accuracy: 94.89%\n",
      "val Loss: 0.2709, Accuracy: 91.28%\n",
      "Completed in 0m 31s\n",
      "-------------- epoch 23 ----------------\n",
      "train Loss: 0.1877, Accuracy: 94.12%\n",
      "val Loss: 0.2939, Accuracy: 90.35%\n",
      "Completed in 0m 30s\n",
      "-------------- epoch 24 ----------------\n",
      "train Loss: 0.1830, Accuracy: 94.41%\n",
      "val Loss: 0.2909, Accuracy: 90.37%\n",
      "Completed in 0m 31s\n",
      "-------------- epoch 25 ----------------\n",
      "train Loss: 0.1432, Accuracy: 95.91%\n",
      "val Loss: 0.2461, Accuracy: 92.14%\n",
      "Completed in 0m 33s\n",
      "-------------- epoch 26 ----------------\n",
      "train Loss: 0.1485, Accuracy: 95.63%\n",
      "val Loss: 0.2615, Accuracy: 91.22%\n",
      "Completed in 0m 31s\n",
      "-------------- epoch 27 ----------------\n",
      "train Loss: 0.1374, Accuracy: 96.28%\n",
      "val Loss: 0.2484, Accuracy: 92.25%\n",
      "Completed in 0m 36s\n",
      "-------------- epoch 28 ----------------\n",
      "train Loss: 0.1114, Accuracy: 97.13%\n",
      "val Loss: 0.2194, Accuracy: 93.02%\n",
      "Completed in 0m 35s\n",
      "-------------- epoch 29 ----------------\n",
      "train Loss: 0.1413, Accuracy: 95.85%\n",
      "val Loss: 0.2609, Accuracy: 91.51%\n",
      "Completed in 0m 32s\n",
      "-------------- epoch 30 ----------------\n",
      "train Loss: 0.1170, Accuracy: 96.70%\n",
      "val Loss: 0.2336, Accuracy: 92.17%\n",
      "Completed in 0m 35s\n"
     ]
    }
   ],
   "source": [
    "base = train_baseline(base_model, train_loader, val_loader, optimizer, EPOCH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(base, 'baseline.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_transforms = {\n",
    "    'train' : \n",
    "\n",
    "}"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "63f079a629f1330562a28709405f9dfec3da39279f0688f168f2fac4b256611f"
  },
  "kernelspec": {
   "display_name": "Python 3.8.13 ('torch_study')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
